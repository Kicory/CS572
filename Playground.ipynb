{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS572 Project\n",
    "* We selected the paper: {}\n",
    "* Based on the data and model of given paper, we are planning to make an extension of LSTM that predicts {}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "data_path = 'Data' # add path\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_epochs = 2000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "weight_decay=0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if your computer is prepared to run pytorch model with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> batch_size, seq, input_size\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "#         out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        # out : batch_size, seq_length, hidden_size\n",
    "        # out (N, 9, 128)  ???\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\\190722_tension_test_space_1_trial_1.csv\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "data_set = []\n",
    "for path_idx in os.listdir(data_path):\n",
    "    cur_path = os.path.join(data_path, path_idx)\n",
    "    print(cur_path)\n",
    "    input_file = np.loadtxt(cur_path, dtype='float', delimiter=',')\n",
    "    # cell = nn.RNN(input_size=4, hidden_size=2, batchfirst=True)\n",
    "    inputs = torch.Tensor(input_file)\n",
    "#     print(inputs[0])\n",
    "    \n",
    "    inputs = inputs[:len(inputs)-len(inputs)%40]\n",
    "#     labels = inputs[:, [5]]\n",
    "# #     print(labels.shape)\n",
    "#     labels = labels.view([-1, 40, 1])\n",
    "    \n",
    "    inputs = inputs[:, [1,2,3,4,6,7,8,9,5]]\n",
    "    inputs = inputs.view([-1,40,9])\n",
    "    \n",
    "#     for test a file\n",
    "    data_set += [inputs]\n",
    "    break\n",
    "   \n",
    "# print(data_set)\n",
    "print(\"load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader setting complete\n"
     ]
    }
   ],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_set[0],\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=data_set[0],\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "print(\"loader setting complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing: To train fingertip forces from input\n",
    "# data, a three-layer BT-LSTM with 128 hidden units is used.\n",
    "# Sequence time length of BT-LSTM was set to T = 40. The final\n",
    "# output of the BT-LSTM layer is fed into the final fully-connected\n",
    "# layer to obtain the one-dimensional output. The mean squared\n",
    "# error (MSE) function is used to measure the loss between the\n",
    "# predicted and the ground truth contact force. During training,\n",
    "# the number of epoch was 2000, and the mini batch size was 32.\n",
    "# Adam optimizer was used with 0.001 learning rate and 10âˆ’5\n",
    "# weight decay\n",
    "lstm = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(train_loader):\n",
    "#     print (i, x)\n",
    "#     print(x.shape)\n",
    "#     inputs, labels = torch.split(x, [8, 1], 2)\n",
    "#     print(inputs.shape)\n",
    "#     print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_loader:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/2000], Loss: 1.2059\n",
      "Epoch [20/2000], Loss: 0.5222\n",
      "Epoch [30/2000], Loss: 0.7755\n",
      "Epoch [40/2000], Loss: 1.7006\n",
      "Epoch [50/2000], Loss: 1.6352\n",
      "Epoch [60/2000], Loss: 0.4719\n",
      "Epoch [70/2000], Loss: 0.3469\n",
      "Epoch [80/2000], Loss: 0.6358\n",
      "Epoch [90/2000], Loss: 1.0986\n",
      "Epoch [100/2000], Loss: 0.7600\n",
      "Epoch [110/2000], Loss: 1.3751\n",
      "Epoch [120/2000], Loss: 0.9058\n",
      "Epoch [130/2000], Loss: 0.7072\n",
      "Epoch [140/2000], Loss: 0.9226\n",
      "Epoch [150/2000], Loss: 0.7239\n",
      "Epoch [160/2000], Loss: 1.9758\n",
      "Epoch [170/2000], Loss: 0.4032\n",
      "Epoch [180/2000], Loss: 0.5901\n",
      "Epoch [190/2000], Loss: 0.7828\n",
      "Epoch [200/2000], Loss: 0.7609\n",
      "Epoch [210/2000], Loss: 0.6584\n",
      "Epoch [220/2000], Loss: 0.7442\n",
      "Epoch [230/2000], Loss: 0.5561\n",
      "Epoch [240/2000], Loss: 0.9940\n",
      "Epoch [250/2000], Loss: 0.6129\n",
      "Epoch [260/2000], Loss: 0.6423\n",
      "Epoch [270/2000], Loss: 1.3106\n",
      "Epoch [280/2000], Loss: 0.5949\n",
      "Epoch [290/2000], Loss: 1.1711\n",
      "Epoch [300/2000], Loss: 0.9932\n",
      "Epoch [310/2000], Loss: 0.6089\n",
      "Epoch [320/2000], Loss: 0.3705\n",
      "Epoch [330/2000], Loss: 0.8942\n",
      "Epoch [340/2000], Loss: 0.2383\n",
      "Epoch [350/2000], Loss: 0.2737\n",
      "Epoch [360/2000], Loss: 0.8723\n",
      "Epoch [370/2000], Loss: 0.9362\n",
      "Epoch [380/2000], Loss: 0.9845\n",
      "Epoch [390/2000], Loss: 0.7177\n",
      "Epoch [400/2000], Loss: 0.9037\n",
      "Epoch [410/2000], Loss: 1.0888\n",
      "Epoch [420/2000], Loss: 0.5954\n",
      "Epoch [430/2000], Loss: 1.0958\n",
      "Epoch [440/2000], Loss: 1.0565\n",
      "Epoch [450/2000], Loss: 0.7427\n",
      "Epoch [460/2000], Loss: 0.3565\n",
      "Epoch [470/2000], Loss: 0.6031\n",
      "Epoch [480/2000], Loss: 1.9633\n",
      "Epoch [490/2000], Loss: 0.7869\n",
      "Epoch [500/2000], Loss: 0.3594\n",
      "Epoch [510/2000], Loss: 0.6473\n",
      "Epoch [520/2000], Loss: 0.6519\n",
      "Epoch [530/2000], Loss: 0.8980\n",
      "Epoch [540/2000], Loss: 0.8133\n",
      "Epoch [550/2000], Loss: 0.4330\n",
      "Epoch [560/2000], Loss: 0.6996\n",
      "Epoch [570/2000], Loss: 0.4241\n",
      "Epoch [580/2000], Loss: 0.5533\n",
      "Epoch [590/2000], Loss: 0.2148\n",
      "Epoch [600/2000], Loss: 0.8646\n",
      "Epoch [610/2000], Loss: 0.5313\n",
      "Epoch [620/2000], Loss: 0.4071\n",
      "Epoch [630/2000], Loss: 0.3403\n",
      "Epoch [640/2000], Loss: 2.0160\n",
      "Epoch [650/2000], Loss: 0.5205\n",
      "Epoch [660/2000], Loss: 0.9250\n",
      "Epoch [670/2000], Loss: 0.9069\n",
      "Epoch [680/2000], Loss: 1.8113\n",
      "Epoch [690/2000], Loss: 0.9294\n",
      "Epoch [700/2000], Loss: 0.5200\n",
      "Epoch [710/2000], Loss: 0.3418\n",
      "Epoch [720/2000], Loss: 0.8231\n",
      "Epoch [730/2000], Loss: 0.2356\n",
      "Epoch [740/2000], Loss: 0.9342\n",
      "Epoch [750/2000], Loss: 0.9063\n",
      "Epoch [760/2000], Loss: 0.5926\n",
      "Epoch [770/2000], Loss: 0.7301\n",
      "Epoch [780/2000], Loss: 0.6800\n",
      "Epoch [790/2000], Loss: 1.3388\n",
      "Epoch [800/2000], Loss: 0.5180\n",
      "Epoch [810/2000], Loss: 0.4204\n",
      "Epoch [820/2000], Loss: 0.7218\n",
      "Epoch [830/2000], Loss: 0.9103\n",
      "Epoch [840/2000], Loss: 0.6187\n",
      "Epoch [850/2000], Loss: 0.5253\n",
      "Epoch [860/2000], Loss: 0.6789\n",
      "Epoch [870/2000], Loss: 0.6928\n",
      "Epoch [880/2000], Loss: 0.4037\n",
      "Epoch [890/2000], Loss: 1.7284\n",
      "Epoch [900/2000], Loss: 0.5374\n",
      "Epoch [910/2000], Loss: 1.1022\n",
      "Epoch [920/2000], Loss: 0.8254\n",
      "Epoch [930/2000], Loss: 0.7382\n",
      "Epoch [940/2000], Loss: 0.6620\n",
      "Epoch [950/2000], Loss: 0.6809\n",
      "Epoch [960/2000], Loss: 0.6349\n",
      "Epoch [970/2000], Loss: 0.4897\n",
      "Epoch [980/2000], Loss: 0.3657\n",
      "Epoch [990/2000], Loss: 0.4593\n",
      "Epoch [1000/2000], Loss: 0.5426\n",
      "Epoch [1010/2000], Loss: 1.0865\n",
      "Epoch [1020/2000], Loss: 0.3605\n",
      "Epoch [1030/2000], Loss: 0.6071\n",
      "Epoch [1040/2000], Loss: 2.5018\n",
      "Epoch [1050/2000], Loss: 1.0326\n",
      "Epoch [1060/2000], Loss: 0.7380\n",
      "Epoch [1070/2000], Loss: 0.7036\n",
      "Epoch [1080/2000], Loss: 1.1098\n",
      "Epoch [1090/2000], Loss: 0.2142\n",
      "Epoch [1100/2000], Loss: 0.6661\n",
      "Epoch [1110/2000], Loss: 0.5396\n",
      "Epoch [1120/2000], Loss: 0.4069\n",
      "Epoch [1130/2000], Loss: 0.5925\n",
      "Epoch [1140/2000], Loss: 0.4349\n",
      "Epoch [1150/2000], Loss: 0.4662\n",
      "Epoch [1160/2000], Loss: 0.7883\n",
      "Epoch [1170/2000], Loss: 1.6942\n",
      "Epoch [1180/2000], Loss: 0.3673\n",
      "Epoch [1190/2000], Loss: 0.5564\n",
      "Epoch [1200/2000], Loss: 1.2057\n",
      "Epoch [1210/2000], Loss: 1.1078\n",
      "Epoch [1220/2000], Loss: 0.2267\n",
      "Epoch [1230/2000], Loss: 0.8517\n",
      "Epoch [1240/2000], Loss: 0.6319\n",
      "Epoch [1250/2000], Loss: 0.2389\n",
      "Epoch [1260/2000], Loss: 0.6503\n",
      "Epoch [1270/2000], Loss: 0.5264\n",
      "Epoch [1280/2000], Loss: 0.6496\n",
      "Epoch [1290/2000], Loss: 0.2390\n",
      "Epoch [1300/2000], Loss: 0.1742\n",
      "Epoch [1310/2000], Loss: 0.5188\n",
      "Epoch [1320/2000], Loss: 0.8917\n",
      "Epoch [1330/2000], Loss: 0.6668\n",
      "Epoch [1340/2000], Loss: 0.5384\n",
      "Epoch [1350/2000], Loss: 0.4676\n",
      "Epoch [1360/2000], Loss: 0.2848\n",
      "Epoch [1370/2000], Loss: 0.3380\n",
      "Epoch [1380/2000], Loss: 0.7285\n",
      "Epoch [1390/2000], Loss: 1.1603\n",
      "Epoch [1400/2000], Loss: 1.5709\n",
      "Epoch [1410/2000], Loss: 0.8167\n",
      "Epoch [1420/2000], Loss: 1.7688\n",
      "Epoch [1430/2000], Loss: 0.7127\n",
      "Epoch [1440/2000], Loss: 0.6103\n",
      "Epoch [1450/2000], Loss: 1.4132\n",
      "Epoch [1460/2000], Loss: 0.9492\n",
      "Epoch [1470/2000], Loss: 0.6321\n",
      "Epoch [1480/2000], Loss: 0.8244\n",
      "Epoch [1490/2000], Loss: 1.4757\n",
      "Epoch [1500/2000], Loss: 0.4849\n",
      "Epoch [1510/2000], Loss: 0.1570\n",
      "Epoch [1520/2000], Loss: 0.8499\n",
      "Epoch [1530/2000], Loss: 0.9636\n",
      "Epoch [1540/2000], Loss: 0.7003\n",
      "Epoch [1550/2000], Loss: 0.9302\n",
      "Epoch [1560/2000], Loss: 0.6437\n",
      "Epoch [1570/2000], Loss: 1.1647\n",
      "Epoch [1580/2000], Loss: 1.1616\n",
      "Epoch [1590/2000], Loss: 0.8044\n",
      "Epoch [1600/2000], Loss: 1.1730\n",
      "Epoch [1610/2000], Loss: 0.8269\n",
      "Epoch [1620/2000], Loss: 1.5007\n",
      "Epoch [1630/2000], Loss: 0.5747\n",
      "Epoch [1640/2000], Loss: 0.6926\n",
      "Epoch [1650/2000], Loss: 0.6277\n",
      "Epoch [1660/2000], Loss: 1.4803\n",
      "Epoch [1670/2000], Loss: 0.5056\n",
      "Epoch [1680/2000], Loss: 1.2015\n",
      "Epoch [1690/2000], Loss: 1.0280\n",
      "Epoch [1700/2000], Loss: 1.0382\n",
      "Epoch [1710/2000], Loss: 0.8956\n",
      "Epoch [1720/2000], Loss: 0.8778\n",
      "Epoch [1730/2000], Loss: 0.9169\n",
      "Epoch [1740/2000], Loss: 0.7896\n",
      "Epoch [1750/2000], Loss: 1.3101\n",
      "Epoch [1760/2000], Loss: 0.7046\n",
      "Epoch [1770/2000], Loss: 0.6450\n",
      "Epoch [1780/2000], Loss: 1.2110\n",
      "Epoch [1790/2000], Loss: 0.7591\n",
      "Epoch [1800/2000], Loss: 0.8425\n",
      "Epoch [1810/2000], Loss: 0.5419\n",
      "Epoch [1820/2000], Loss: 0.5738\n",
      "Epoch [1830/2000], Loss: 0.7996\n",
      "Epoch [1840/2000], Loss: 1.2867\n",
      "Epoch [1850/2000], Loss: 0.6735\n",
      "Epoch [1860/2000], Loss: 0.9833\n",
      "Epoch [1870/2000], Loss: 1.0902\n",
      "Epoch [1880/2000], Loss: 0.6579\n",
      "Epoch [1890/2000], Loss: 1.2175\n",
      "Epoch [1900/2000], Loss: 0.3123\n",
      "Epoch [1910/2000], Loss: 0.4421\n",
      "Epoch [1920/2000], Loss: 0.5741\n",
      "Epoch [1930/2000], Loss: 0.8162\n",
      "Epoch [1940/2000], Loss: 0.4108\n",
      "Epoch [1950/2000], Loss: 0.5536\n",
      "Epoch [1960/2000], Loss: 1.0334\n",
      "Epoch [1970/2000], Loss: 0.9358\n",
      "Epoch [1980/2000], Loss: 0.4807\n",
      "Epoch [1990/2000], Loss: 0.2201\n",
      "Epoch [2000/2000], Loss: 0.8584\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "# for epoch in range(1):    \n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        inputs, labels = torch.split(inputs, [8, 1], 2)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels[:,[39],:].view([-1,1])\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lstm(inputs)\n",
    "        \n",
    "#         print(labels.shape)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3944323acac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mrmse\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mmae\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \"\"\"\n\u001b[1;32m--> 255\u001b[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[0;32m    256\u001b[0m         y_true, y_pred, multioutput)\n\u001b[0;32m    257\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\CS572\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    rmse = 0\n",
    "    mae = 0\n",
    "    for inputs in test_loader:\n",
    "        inputs, labels = torch.split(inputs, [8, 1], 2)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels[:,[39],:].view([-1,1])\n",
    "        labels = labels.to(device)\n",
    "        outputs = lstm(inputs)\n",
    "        \n",
    "        rmse += mean_squared_error(labels, outputs)\n",
    "        mae += mean_absolute_error(labels, outputs)\n",
    "        \n",
    "    rmse = sqrt(rmse)\n",
    "\n",
    "    print('Test Accuracy of the model RMSE: {} %'.format(rmse))\n",
    "    print('Test Accuracy of the model MAE: {} %'.format(mae)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
