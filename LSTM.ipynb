{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS572 Project\n",
    "* We selected the paper: {}\n",
    "* Based on the data and model of given paper, we are planning to make an extension of LSTM that predicts {}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_epochs = 2000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "weight_decay=0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if your computer is prepared to run pytorch model with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # x -> batch_size, seq, input_size\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def makehc_zero(self, batch_size):\n",
    "        self.hn = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        self.cn = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "#         print(\"clear\")\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training == True:\n",
    "            # Set initial hidden and cell states \n",
    "            self.makehc_zero(x.size(0))\n",
    "#         out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (self.hn, self.cn) = self.lstm(x, (self.hn, self.cn))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        # out : batch_size, seq_length, hidden_size\n",
    "        # out (N, 9, 128)  ???\n",
    "        \n",
    "        out_seq = [self.fc(out[:,i,:]) for i in range(out.size(1))]\n",
    "        out = torch.cat(out_seq, dim=1)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\\Training\\190722_tension_test_space_1_trial_2.csv\n",
      "Data\\Training\\190722_tension_test_space_1_trial_3.csv\n",
      "Data\\Training\\190722_tension_test_space_2_trial_1.csv\n",
      "Data\\Training\\190722_tension_test_space_2_trial_2.csv\n",
      "Data\\Training\\190722_tension_test_space_2_trial_4.csv\n",
      "Data\\Training\\190722_tension_test_space_3_trial_2.csv\n",
      "Data\\Training\\190722_tension_test_space_3_trial_3.csv\n",
      "Data\\Training\\190722_tension_test_space_3_trial_4.csv\n",
      "Data\\Training\\190722_tension_test_space_3_trial_5.csv\n",
      "Data\\Training\\190722_tension_test_space_3_trial_6.csv\n",
      "Data\\Training\\190722_tension_test_space_4_trial_2.csv\n",
      "Data\\Training\\190722_tension_test_space_4_trial_3.csv\n",
      "Data\\Training\\190722_tension_test_space_5_trial_2.csv\n",
      "Data\\Training\\190722_tension_test_space_5_trial_3.csv\n",
      "Data\\Training\\test11thick_1.csv\n",
      "Data\\Training\\test11thick_3.csv\n",
      "Data\\Training\\test22thick_2.csv\n",
      "Data\\Training\\test33thick_2.csv\n",
      "Data\\Training\\test33thick_3.csv\n",
      "Data\\Training\\test55thick_2.csv\n",
      "Data\\Training\\test55thick_3.csv\n",
      "Data\\Testing\\190722_tension_test_space_1_trial_1.csv\n",
      "Data\\Testing\\190722_tension_test_space_2_trial_3.csv\n",
      "Data\\Testing\\190722_tension_test_space_3_trial_1.csv\n",
      "Data\\Testing\\190722_tension_test_space_4_trial_1.csv\n",
      "Data\\Testing\\190722_tension_test_space_5_trial_1.csv\n",
      "Data\\Testing\\test11thick_2.csv\n",
      "Data\\Testing\\test22thick_1.csv\n",
      "Data\\Testing\\test33thick_1.csv\n",
      "Data\\Testing\\test44thick_1.csv\n",
      "Data\\Testing\\test55thick_1.csv\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "training_set = []\n",
    "data_path = 'Data\\Training' # add path\n",
    "\n",
    "for path_idx in os.listdir(data_path):\n",
    "    cur_path = os.path.join(data_path, path_idx)\n",
    "    print(cur_path)\n",
    "    input_file = np.loadtxt(cur_path, dtype='float', delimiter=',')\n",
    "    # cell = nn.RNN(input_size=4, hidden_size=2, batchfirst=True)\n",
    "    inputs = torch.Tensor(input_file)\n",
    "#     print(inputs[0])\n",
    "    \n",
    "    inputs = inputs[:len(inputs)-len(inputs)%40]\n",
    "#     labels = inputs[:, [5]]\n",
    "# #     print(labels.shape)\n",
    "#     labels = labels.view([-1, 40, 1])\n",
    "    \n",
    "    inputs = inputs[:, [1,2,3,4,6,7,8,9,5]]\n",
    "    inputs = inputs.view([-1,40,9])\n",
    "    \n",
    "#     for test a file\n",
    "    training_set += inputs\n",
    "\n",
    "testing_set = []\n",
    "data_path = 'Data\\Testing' # add path\n",
    "for path_idx in os.listdir(data_path):\n",
    "    cur_path = os.path.join(data_path, path_idx)\n",
    "    print(cur_path)\n",
    "    input_file = np.loadtxt(cur_path, dtype='float', delimiter=',')\n",
    "    # cell = nn.RNN(input_size=4, hidden_size=2, batchfirst=True)\n",
    "    inputs = torch.Tensor(input_file)\n",
    "#     print(inputs[0])\n",
    "    \n",
    "    inputs = inputs[:len(inputs)-len(inputs)%40]\n",
    "#     labels = inputs[:, [5]]\n",
    "# #     print(labels.shape)\n",
    "#     labels = labels.view([-1, 40, 1])\n",
    "    \n",
    "    inputs = inputs[:, [1,2,3,4,6,7,8,9,5]]\n",
    "    inputs = inputs.view([-1,40,9])\n",
    "    \n",
    "#     for test a file\n",
    "    testing_set += inputs\n",
    "   \n",
    "# print(data_set)\n",
    "print(\"load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader setting complete\n"
     ]
    }
   ],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_set,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_set,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)\n",
    "print(\"loader setting complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing: To train fingertip forces from input\n",
    "# data, a three-layer BT-LSTM with 128 hidden units is used.\n",
    "# Sequence time length of BT-LSTM was set to T = 40. The final\n",
    "# output of the BT-LSTM layer is fed into the final fully-connected\n",
    "# layer to obtain the one-dimensional output. The mean squared\n",
    "# error (MSE) function is used to measure the loss between the\n",
    "# predicted and the ground truth contact force. During training,\n",
    "# the number of epoch was 2000, and the mini batch size was 32.\n",
    "# Adam optimizer was used with 0.001 learning rate and 10âˆ’5\n",
    "# weight decay\n",
    "lstm = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cell if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.load_state_dict(torch.load('LSTM.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(train_loader):\n",
    "#     print (i, x)\n",
    "#     print(x.shape)\n",
    "#     inputs, labels = torch.split(x, [8, 1], 2)\n",
    "#     print(inputs.shape)\n",
    "#     print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_loader:\n",
    "    print (i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "Epoch [10/2000], Loss: 3.5906\n",
      "elapsed time : 72.07684373855591\n",
      "Epoch [20/2000], Loss: 1.5470\n",
      "elapsed time : 144.9637417793274\n",
      "Epoch [30/2000], Loss: 1.5745\n",
      "elapsed time : 217.28625869750977\n",
      "Epoch [40/2000], Loss: 2.2112\n",
      "elapsed time : 290.08675932884216\n",
      "Epoch [50/2000], Loss: 0.4893\n",
      "elapsed time : 363.2852544784546\n",
      "Epoch [60/2000], Loss: 2.1556\n",
      "elapsed time : 435.5586516857147\n",
      "Epoch [70/2000], Loss: 0.9105\n",
      "elapsed time : 508.458904504776\n",
      "Epoch [80/2000], Loss: 2.8663\n",
      "elapsed time : 580.8677232265472\n",
      "Epoch [90/2000], Loss: 5.0020\n",
      "elapsed time : 653.1193611621857\n",
      "Epoch [100/2000], Loss: 0.1652\n",
      "elapsed time : 724.5843496322632\n",
      "Epoch [110/2000], Loss: 0.8182\n",
      "elapsed time : 796.7147650718689\n",
      "Epoch [120/2000], Loss: 0.6453\n",
      "elapsed time : 868.1304385662079\n",
      "Epoch [130/2000], Loss: 1.3649\n",
      "elapsed time : 939.9476895332336\n",
      "Epoch [140/2000], Loss: 1.1188\n",
      "elapsed time : 1011.8189153671265\n",
      "Epoch [150/2000], Loss: 2.1779\n",
      "elapsed time : 1084.1855487823486\n",
      "Epoch [160/2000], Loss: 1.2901\n",
      "elapsed time : 1156.301726102829\n",
      "Epoch [170/2000], Loss: 1.0349\n",
      "elapsed time : 1229.6294739246368\n",
      "Epoch [180/2000], Loss: 2.8888\n",
      "elapsed time : 1302.1338059902191\n",
      "Epoch [190/2000], Loss: 1.0033\n",
      "elapsed time : 1374.8128745555878\n",
      "Epoch [200/2000], Loss: 3.7284\n",
      "elapsed time : 1447.2683918476105\n",
      "Epoch [210/2000], Loss: 1.1994\n",
      "elapsed time : 1518.6335775852203\n",
      "Epoch [220/2000], Loss: 0.4932\n",
      "elapsed time : 1590.490432024002\n",
      "Epoch [230/2000], Loss: 2.1850\n",
      "elapsed time : 1661.8416607379913\n",
      "Epoch [240/2000], Loss: 0.8775\n",
      "elapsed time : 1734.1513211727142\n",
      "Epoch [250/2000], Loss: 0.7676\n",
      "elapsed time : 1805.3656430244446\n",
      "Epoch [260/2000], Loss: 1.1366\n",
      "elapsed time : 1877.1234757900238\n",
      "Epoch [270/2000], Loss: 0.8043\n",
      "elapsed time : 1948.6594145298004\n",
      "Epoch [280/2000], Loss: 0.5275\n",
      "elapsed time : 2020.646785736084\n",
      "Epoch [290/2000], Loss: 2.4226\n",
      "elapsed time : 2092.6423633098602\n",
      "Epoch [300/2000], Loss: 0.2167\n",
      "elapsed time : 2164.6015315055847\n",
      "Epoch [310/2000], Loss: 2.4012\n",
      "elapsed time : 2239.9500966072083\n",
      "Epoch [320/2000], Loss: 3.6349\n",
      "elapsed time : 2314.9789638519287\n",
      "Epoch [330/2000], Loss: 0.8667\n",
      "elapsed time : 2386.0662920475006\n",
      "Epoch [340/2000], Loss: 1.9954\n",
      "elapsed time : 2457.6148262023926\n",
      "Epoch [350/2000], Loss: 0.7309\n",
      "elapsed time : 2528.794026374817\n",
      "Epoch [360/2000], Loss: 4.8746\n",
      "elapsed time : 2600.9891335964203\n",
      "Epoch [370/2000], Loss: 0.5664\n",
      "elapsed time : 2672.307785987854\n",
      "Epoch [380/2000], Loss: 1.0638\n",
      "elapsed time : 2744.226408481598\n",
      "Epoch [390/2000], Loss: 1.3423\n",
      "elapsed time : 2820.74627494812\n",
      "Epoch [400/2000], Loss: 0.4572\n",
      "elapsed time : 2892.1484940052032\n",
      "Epoch [410/2000], Loss: 3.8631\n",
      "elapsed time : 2965.620845556259\n",
      "Epoch [420/2000], Loss: 0.5909\n",
      "elapsed time : 3039.7581741809845\n",
      "Epoch [430/2000], Loss: 0.3952\n",
      "elapsed time : 3111.7745990753174\n",
      "Epoch [440/2000], Loss: 1.4376\n",
      "elapsed time : 3193.3086342811584\n",
      "Epoch [450/2000], Loss: 0.4742\n",
      "elapsed time : 3268.6189427375793\n",
      "Epoch [460/2000], Loss: 0.5589\n",
      "elapsed time : 3340.285935163498\n",
      "Epoch [470/2000], Loss: 0.9466\n",
      "elapsed time : 3411.273461818695\n",
      "Epoch [480/2000], Loss: 2.9419\n",
      "elapsed time : 3487.8670427799225\n",
      "Epoch [490/2000], Loss: 1.8489\n",
      "elapsed time : 3560.650670528412\n",
      "Epoch [500/2000], Loss: 1.3359\n",
      "elapsed time : 3630.7334752082825\n",
      "Epoch [510/2000], Loss: 2.0198\n",
      "elapsed time : 3711.4692549705505\n",
      "Epoch [520/2000], Loss: 2.3202\n",
      "elapsed time : 3794.4261837005615\n",
      "Epoch [530/2000], Loss: 0.7521\n",
      "elapsed time : 3877.351535320282\n",
      "Epoch [540/2000], Loss: 2.9803\n",
      "elapsed time : 3952.831509590149\n",
      "Epoch [550/2000], Loss: 0.4743\n",
      "elapsed time : 4035.7364587783813\n",
      "Epoch [560/2000], Loss: 1.1957\n",
      "elapsed time : 4115.828984498978\n",
      "Epoch [570/2000], Loss: 1.2617\n",
      "elapsed time : 4192.543137311935\n",
      "Epoch [580/2000], Loss: 0.4030\n",
      "elapsed time : 4265.262483596802\n",
      "Epoch [590/2000], Loss: 2.6426\n",
      "elapsed time : 4337.782706022263\n",
      "Epoch [600/2000], Loss: 1.0703\n",
      "elapsed time : 4413.38777089119\n",
      "Epoch [610/2000], Loss: 1.5864\n",
      "elapsed time : 4500.824267864227\n",
      "Epoch [620/2000], Loss: 1.8053\n",
      "elapsed time : 4576.974830389023\n",
      "Epoch [630/2000], Loss: 0.7890\n",
      "elapsed time : 4648.873502731323\n",
      "Epoch [640/2000], Loss: 0.8619\n",
      "elapsed time : 4721.365223169327\n",
      "Epoch [650/2000], Loss: 0.9594\n",
      "elapsed time : 4793.282945156097\n",
      "Epoch [660/2000], Loss: 4.0574\n",
      "elapsed time : 4872.40224146843\n",
      "Epoch [670/2000], Loss: 6.3784\n",
      "elapsed time : 4945.5711143016815\n",
      "Epoch [680/2000], Loss: 0.7096\n",
      "elapsed time : 5017.303849935532\n",
      "Epoch [690/2000], Loss: 0.2794\n",
      "elapsed time : 5090.6499927043915\n",
      "Epoch [700/2000], Loss: 0.5413\n",
      "elapsed time : 5162.432788133621\n",
      "Epoch [710/2000], Loss: 0.2708\n",
      "elapsed time : 5238.2814774513245\n",
      "Epoch [720/2000], Loss: 0.8929\n",
      "elapsed time : 5311.15928030014\n",
      "Epoch [730/2000], Loss: 2.6602\n",
      "elapsed time : 5389.189425945282\n",
      "Epoch [740/2000], Loss: 0.2599\n",
      "elapsed time : 5461.879466056824\n",
      "Epoch [750/2000], Loss: 2.3254\n",
      "elapsed time : 5533.400819778442\n",
      "Epoch [760/2000], Loss: 0.8922\n",
      "elapsed time : 5604.676750421524\n",
      "Epoch [770/2000], Loss: 1.1797\n",
      "elapsed time : 5677.079803466797\n",
      "Epoch [780/2000], Loss: 4.2690\n",
      "elapsed time : 5749.405957460403\n",
      "Epoch [790/2000], Loss: 1.1535\n",
      "elapsed time : 5821.876756906509\n",
      "Epoch [800/2000], Loss: 3.6755\n",
      "elapsed time : 5894.169108867645\n",
      "Epoch [810/2000], Loss: 0.8708\n",
      "elapsed time : 5971.2052183151245\n",
      "Epoch [820/2000], Loss: 3.3623\n",
      "elapsed time : 6048.494489431381\n",
      "Epoch [830/2000], Loss: 0.9448\n",
      "elapsed time : 6120.917279481888\n",
      "Epoch [840/2000], Loss: 0.9662\n",
      "elapsed time : 6191.850818634033\n",
      "Epoch [850/2000], Loss: 6.7839\n",
      "elapsed time : 6264.1767592430115\n",
      "Epoch [860/2000], Loss: 1.0376\n",
      "elapsed time : 6335.554305315018\n",
      "Epoch [870/2000], Loss: 1.1856\n",
      "elapsed time : 6408.524714231491\n",
      "Epoch [880/2000], Loss: 1.1903\n",
      "elapsed time : 6480.751647949219\n",
      "Epoch [890/2000], Loss: 1.7380\n",
      "elapsed time : 6552.2779223918915\n",
      "Epoch [900/2000], Loss: 0.2940\n",
      "elapsed time : 6624.988421916962\n",
      "Epoch [910/2000], Loss: 0.5781\n",
      "elapsed time : 6697.082388162613\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start = time.time()\n",
    "print(\"training start\")\n",
    "for epoch in range(num_epochs):\n",
    "# for epoch in range(200):    \n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        inputs, labels = torch.split(inputs, [8, 1], 2)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lstm(inputs)\n",
    "        \n",
    "#         print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, loss.item()))\n",
    "        print('elapsed time :', time.time() - start)\n",
    "print(\"training complete\")\n",
    "print('total_training elapsed time :', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.eval()\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    total_labels = []\n",
    "    total_outputs = []\n",
    "    lstm.makehc_zero(1) #test batch size = 1\n",
    "    for inputs in test_loader:\n",
    "        inputs, labels = torch.split(inputs, [8, 1], 2)\n",
    "        \n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze()\n",
    "        \n",
    "        for i in range(inputs.size(1)):\n",
    "            outputs = lstm(inputs[:,i,:].view([inputs.size(0),1,8]))\n",
    "            total_outputs += outputs.cpu()\n",
    "        \n",
    "        total_labels += labels\n",
    "    rmse = mean_squared_error(total_labels, total_outputs, squared = False)\n",
    "    mae = mean_absolute_error(total_labels, total_outputs)\n",
    "\n",
    "    print('Test Accuracy of the model RMSE: {}'.format(rmse))\n",
    "    print('Test Accuracy of the model MAE: {}'.format(mae)) \n",
    "    print('testing elapsed time :', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), 'LSTM_2000epoch.ckpt')\n",
    "print(\"save complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_outputs)\n",
    "plt.plot(total_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_outputs)\n",
    "plt.plot(total_labels)\n",
    "plt.savefig('LSTM_compare_all_labels_2000epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
